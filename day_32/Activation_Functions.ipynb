{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Activation Functions Crash Course in 75 Minutes | Deep Learning with Tensorflow\n",
    "\n",
    "Written by: M.Danish Azeem\\\n",
    "Date: 02.10.2024\\\n",
    "Email: danishazeem365@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment #(1)  \n",
    " \n",
    "Time stamp   58:05\n",
    "\n",
    "# what are the difference between SoftMax and sigmoid function\n",
    "\n",
    "\n",
    "Both SoftMax and Sigmoid functions are activation functions used in neural networks, but they have distinct functionalities and applications:\n",
    "\n",
    "**Sigmoid Function:**\n",
    "\n",
    "* **Output:** Values between 0 and 1, representing probabilities for a **single class**.\n",
    "* **Applications:** Useful for binary classification problems (e.g., predicting if an image is a cat or not a cat).\n",
    "* **Interpretation:** Output closer to 1 indicates higher probability of belonging to the positive class, closer to 0 indicates lower probability.\n",
    "\n",
    "**SoftMax Function:**\n",
    "\n",
    "* **Output:** Values between 0 and 1, representing **probabilities for all possible classes**, where values sum to 1.\n",
    "* **Applications:** Used for **multi-class classification problems** (e.g., predicting what type of clothing is in an image).\n",
    "* **Interpretation:** Highest output value corresponds to the class with the highest probability.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Feature | Sigmoid | SoftMax |\n",
    "|---|---|---|\n",
    "| Output range | 0-1 | 0-1, sum to 1 |\n",
    "| Number of classes | Single | Multiple |\n",
    "| Applications | Binary classification | Multi-class classification |\n",
    "| Interpretation | Probability for one class | Probabilities for all classes relative to each other |\n",
    "\n",
    "**Choosing the right function:**\n",
    "\n",
    "* Use Sigmoid for predicting a single class (e.g., is it spam or not?).\n",
    "* Use SoftMax for predicting one class from multiple options (e.g., cat, dog, bird).\n",
    "\n",
    "**Additional notes:**\n",
    "\n",
    "* Sigmoid can be used for multi-class problems by applying it independently to each output neuron, but SoftMax is generally preferred for its efficiency and better compatibility with cross-entropy loss functions.\n",
    "* Other activation functions exist, each with its own strengths and weaknesses depending on the specific task and network architecture.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment #(2)  \n",
    " \n",
    "Time stamp   58:05\n",
    "\n",
    "# How we do know, which activation function should we use in hidden or out put layers?\n",
    "\n",
    "\n",
    "Choosing the right activation function for hidden and output layers in neural networks is an important decision that can significantly impact your model's performance. Here are some key factors to consider:\n",
    "\n",
    "**Hidden Layers:**\n",
    "\n",
    "* **Non-linearity:** In most cases, you want non-linear activation functions in hidden layers. This allows the network to learn complex relationships and patterns in the data, as a linear model (with linear activations) can only represent linear relationships.\n",
    "* **Common choices:** ReLU (Rectified Linear Unit) is the most popular choice due to its efficiency and vanishing gradient mitigation. Other options include Leaky ReLU, ELU (Exponential Linear Unit), and Tanh (Hyperbolic Tangent).\n",
    "* **Considerations:** ReLU may die with negative values, Leaky ReLU addresses this, while ELU and Tanh have bounded outputs (-1 to 1 for Tanh). Experimentation is often necessary to find the best option for your specific problem.\n",
    "\n",
    "**Output Layer:**\n",
    "\n",
    "* **Output range:** The activation function should match the expected output range of your problem.\n",
    "* **Binary classification:** Use Sigmoid when the output is a single probability between 0 and 1 (e.g., is it spam or not?).\n",
    "* **Multi-class classification:** Use SoftMax when you have multiple mutually exclusive classes, and the outputs represent probabilities for each class, summing to 1 (e.g., predicting the type of clothing in an image).\n",
    "* **Regression:** Use a linear activation function (no activation) for continuous outputs (e.g., predicting house prices).\n",
    "\n",
    "**Additional Tips:**\n",
    "\n",
    "* Consider the activation function's impact on the loss function used during training, as some combinations work better together.\n",
    "* Experiment with different options and evaluate your model's performance on both training and validation data to find the best fit.\n",
    "* Pay attention to potential issues like vanishing/exploding gradients with certain activation functions and choose accordingly.\n",
    "\n",
    "Remember, there's no one-size-fits-all answer. The optimal activation function depends on your specific task, dataset, and network architecture. Be prepared to experiment and compare different options to achieve the best results for your model!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment #(3)  \n",
    " \n",
    "Time stamp   01:10:41\n",
    "\n",
    "# what are mutually exclusive and mutually inclusive?\n",
    "\n",
    "\n",
    "Mutually exclusive and mutually inclusive are terms used to describe the relationship between events or sets. Here's a breakdown:\n",
    "\n",
    "**Mutually exclusive:**\n",
    "\n",
    "* Events or sets are **mutually exclusive** if they cannot occur or belong to each other **at the same time**. They are like disjoint circles that never overlap.\n",
    "* Examples:\n",
    "    * Rolling a 6 on a die and rolling a 1 on the same die are mutually exclusive (can't happen simultaneously).\n",
    "    * Being a mammal and being a bird are mutually exclusive (you can't be both).\n",
    "    * Sets {even numbers} and {odd numbers} are mutually exclusive (no number can be both even and odd).\n",
    "\n",
    "**Mutually inclusive:**\n",
    "\n",
    "* Events or sets are **mutually inclusive** if they are not **necessarily** exclusive, meaning they **can potentially overlap** or even be identical.\n",
    "* Examples:\n",
    "    * Picking a red card and picking a diamond card from a deck are not mutually exclusive (both can be red diamonds).\n",
    "    * Enjoying music and enjoying movies are not mutually exclusive (someone can enjoy both).\n",
    "    * Sets {numbers less than 5} and {even numbers} are mutually inclusive (some numbers like 2 are in both sets).\n",
    "\n",
    "**Key differences:**\n",
    "\n",
    "| Feature | Mutually exclusive | Mutually inclusive |\n",
    "|---|---|---|\n",
    "| Overlap | No overlap | Can overlap |\n",
    "| Occurrence | Cannot occur together | Can occur together |\n",
    "| Example | Rolling even vs. odd | Enjoying music vs. movies |\n",
    "\n",
    "**Importance:**\n",
    "\n",
    "* Understanding these concepts is crucial in various fields, including probability, statistics, logic, and machine learning.\n",
    "* They help analyze events, classify data, and make informed decisions based on potential occurrences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment #(4)  \n",
    " \n",
    "Time stamp   01:13:35\n",
    "\n",
    "# Write down the mathematical formula, advantages and disadvantages of each activation function.\n",
    "\n",
    "\n",
    "\n",
    "## Activation Functions in Neural Networks: Formulas, Advantages, and Disadvantages\n",
    "\n",
    "Here's a breakdown of the common activation functions you mentioned, including their formulas, advantages, and disadvantages:\n",
    "\n",
    "**1. Linear Activation Function:**\n",
    "\n",
    "* **Formula:** `f(x) = x`\n",
    "* **Advantages:**\n",
    "    * Simple and computationally efficient.\n",
    "    * Preserves all information from the input.\n",
    "* **Disadvantages:**\n",
    "    * Limited expressiveness due to linearity.\n",
    "    * Can suffer from vanishing gradient problem.\n",
    "    * Not suitable for non-linear problems.\n",
    "\n",
    "**2. Step/Binary Activation Function:**\n",
    "\n",
    "* **Formula:** `f(x) = { 1 if x >= 0, 0 if x < 0 }`\n",
    "* **Advantages:**\n",
    "    * Simple and interpretable.\n",
    "    * Useful for binary classification tasks.\n",
    "* **Disadvantages:**\n",
    "    * Creates hard boundaries, potentially losing information.\n",
    "    * Cannot be used for complex problems requiring smooth transitions.\n",
    "\n",
    "**3. Sigmoid Function:**\n",
    "\n",
    "* **Formula:** `f(x) = 1 / (1 + exp(-x))`\n",
    "* **Advantages:**\n",
    "    * Outputs values between 0 and 1, suitable for probability interpretation.\n",
    "    * Smooth and differentiable.\n",
    "* **Disadvantages:**\n",
    "    * Computationally expensive compared to ReLU.\n",
    "    * Can suffer from vanishing gradient problem for large negative inputs.\n",
    "    * Outputs saturate near 0 and 1, limiting learning capacity in deeper layers.\n",
    "\n",
    "**4. Tanh Function:**\n",
    "\n",
    "* **Formula:** `f(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))`\n",
    "* **Advantages:**\n",
    "    * Outputs values between -1 and 1, useful for bipolar problems.\n",
    "    * Smooth and differentiable.\n",
    "    * Zero-centered, avoiding vanishing gradient problem for small inputs.\n",
    "* **Disadvantages:**\n",
    "    * Similar computational cost to Sigmoid.\n",
    "    * Outputs still saturate near -1 and 1, limiting learning capacity.\n",
    "\n",
    "**5. ReLU Function:**\n",
    "\n",
    "* **Formula:** `f(x) = max(0, x)`\n",
    "* **Advantages:**\n",
    "    * Simple and computationally efficient.\n",
    "    * Does not suffer from vanishing gradient problem.\n",
    "    * Widely used due to its effectiveness in many tasks.\n",
    "* **Disadvantages:**\n",
    "    * Can die (output 0) for negative inputs, potentially losing information.\n",
    "    * May lead to the \"dead ReLU\" problem if many neurons die simultaneously.\n",
    "\n",
    "**6. Extended ReLU Functions:**\n",
    "\n",
    "* **Leaky ReLU:** `f(x) = max(alpha * x, x)` (where alpha is a small positive value)\n",
    "* **Parametric ReLU:** `f(x) = max(a * x + b, x)` (where a and b are learned parameters)\n",
    "* **Advantages:**\n",
    "    * Address the \"dead ReLU\" problem by allowing small positive gradients for negative inputs.\n",
    "    * Parametric ReLU offers more flexibility in learning the activation function.\n",
    "* **Disadvantages:**\n",
    "    * Introduce additional hyperparameters that need tuning.\n",
    "    * May be slightly less computationally efficient than ReLU.\n",
    "\n",
    "**7. Softmax Function:**\n",
    "\n",
    "* **Formula:** `f_i(x) = exp(x_i) / sum(exp(x_j)) for all i, j`\n",
    "* **Advantages:**\n",
    "    * Outputs a probability distribution for multi-class classification.\n",
    "    * Ensures outputs sum to 1, representing mutually exclusive classes.\n",
    "* **Disadvantages:**\n",
    "    * Computationally expensive compared to ReLU.\n",
    "    * Numerically unstable for large inputs, requiring careful implementation.\n",
    "\n",
    "**Choosing the right activation function depends on your specific task, dataset, and network architecture.** Experiment with different options and evaluate your model's performance to find the best fit!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
